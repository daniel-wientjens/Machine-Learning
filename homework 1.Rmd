---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r}
library(CVXR)
library(leaps)
library(ggplot2)
library(caret)
```
**Ex 1.**
1.
a) Reading in the data
```{r}
data <- read.table("http://statweb.stanford.edu/~tibs/ElemStatLearn.1stEd/datasets/prostate.data", sep = "")
```

```{r}
data[32,2] = 3.8044  # strange science 
```
b) Extract and normalize the explicative variables
```{r}
X <- scale(data[,1:8])
```
c) Is it wise to normalize these data?
d)Extract the target variable
```{r}
Y <- as.matrix(data[,"lpsa"])
```

e) Split the dataset into training and test data
```{r}
Xtrain <- X[data[["train"]], ]
Ytrain <- Y[data[["train"]], ]

Xtest <- X[!data[["train"]], ]
Ytest <- Y[!data[["train"]], ] %>% as.matrix()
```

**2. Compute the correlations of predictors in the prostate cancer data as presented Table 3.1**
```{r}
Xtrainscale <- scale(Xtrain)
C <- cov(as.matrix(Xtrainscale))
```
3. Reproduce the results presented Table 3.2
a) Compute the coefficients of the linear regression model, without using the lm function (but you can use it validate your code)
```{r}
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain)
b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
```
Now we produce the linear regression model to compare
```{r}
lm0 <- lm(Ytrain ~Xtrain)
comp <-cbind(b,lm0$coefficients)
```

b) Compute the prediction error
```{r}
Ypred <- Xtrainone %*% b
err <- Ytrain - Ypred
```
c) Compute the standard error for each variable
```{r}
sig2 <- (t(err) %*% err)/ (nrow(Xtrainone) - ncol(X) -1)
v <- diag(solve(t(Xtrainone) %*% Xtrainone))
stderr <- sqrt(as.vector(sig2)) * sqrt(v)
```
d) compute the Z score for each variable
```{r}
Z <- b/stderr
```
e) visualize the results and compare with table 3.2
```{r}
table32 <- cbind(b,stderr,Z)
round(100*table32)/100
```


**Ex 2.**
Reproduce Table 3.3, at least the first four columns that is LS, Best Subset, Ridge and Lasso.

**LS column**
```{r}
# CVXR: An R Package for Disciplined Convex Optimization
p <- 9
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)
bo <- round(result$getValue(betaHat), 3)
#Lastly we compare the found bo model with the b model from exercise 1 to see if the values match up
cbind((round(1000*bo)/1000),(round(1000*b)/1000))
```


**Best subset column**
```{r}
#Here we use the leaps package to find the best subset using an exhaustive search
df <- as.data.frame(cbind(Xtrain,Ytrain))
best.subset <- regsubsets(Ytrain~., df)
best.subset.summary <- summary(best.subset)
best.subset.summary$outmat
```
Since we already know that we only need two explanatory variables we use 'lcavol' and 'lweight' according to the table since they have the stars in the second row.

Just to be sure we should only use two explanatory variables we can check some information criteria.

In the end we choose the Bayesian Information Criterion and indeed we can see below that we only need two explanatory variables since it minimizes the BIC.
The adjusted R^2 shows us that it is maximized when we choose 7 variables and explains 66% of the variance, if we choose only 2 explanatory variables we explain roughly 60% of the variance with the benefit of a lot less complexity due to only having 2 variables hence we choose 2 variables as the best subset model.
Lastly looking at Mallow's CP since we use LS once we found the best variables, here we also see that it defines the best subset model as the one containing 7 explanatory variables. But since we have 8 parameters in the full model we can say that there was a sampling error and hence we disregard this.

```{r}
best.subset.by.bic <- which.min(best.subset.summary$bic)
best.subset.by.cp <- which.min(best.subset.summary$cp)
best.subset.by.adjr2 <- which.max(best.subset.summary$adjr2)

par(mfrow=c(2,2))
plot(best.subset$rss, xlab="Number of Variables", ylab="RSS", type="l")
plot(best.subset.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
points(best.subset.by.adjr2, best.subset.summary$adjr2[best.subset.by.adjr2], col="red", cex =2, pch =20)
plot(best.subset.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
points(best.subset.by.bic, best.subset.summary$bic[best.subset.by.bic], col="red", cex =2, pch =20)
plot(best.subset.summary$cp, xlab="Number of Variables", ylab="CP", type="l")
points(best.subset.by.cp, best.subset.summary$cp[best.subset.by.cp], col="red", cex =2, pch =20)
```
Next we use the same method employed during the LS model to find our coefficients only this time we use just the two explanatory variables the best subset model gave us.

```{r}
p <- 3
betaHat <- Variable(p)
Xtrainone <- Xtrainone[,1:3]
objective <- Minimize(sum((Ytrain  - Xtrainone %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)
bb <- result$getValue(betaHat)

#Lastly we compare the found bb model with the lm1 model which used a linear regression on only the two explanatory variables found
lm1 <- lm(Ytrain ~Xtrain[,1:2])
comp <-cbind(bb,lm1$coefficients)
round(1000*comp)/1000
bb <- round(rbind(bb, 0, 0, 0, 0, 0, 0),3)
```

**Ridge column**
```{r}
#First off we centre the Xtrain data to account for the centration
Xtraincen <- Xtrain-mean(Xtrain)
br <- solve(t(Xtraincen) %*% Xtraincen + diag(x = 24, ncol(Xtraincen)), t(Xtraincen) %*% (Ytrain - mean(Ytrain)))
br <- rbind(mean(Ytrain),br)
br <- round(br,3)
```


**The Lasso**
```{r}
Xtrainscale <- scale(Xtrain)

t <-  .7015
ys = scale(Ytrain)
betaHat <- Variable(dim(Xtrainscale)[2])
objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
constraint <- list(sum(abs(betaHat)) <=  t)
problem <- Problem(objective, constraint)
result <- solve(problem)

d <- sqrt(diag(var(Xtrain)))
bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
bl <- round(rbind(mean(Ypred-(Xtrain %*% bl)),bl),3)
```



**PCR column**

```{r}
train <- as.data.frame(cbind(Ytrain, Xtrain))
pcr_model <- pcr(Ytrain ~ ., data = train, validation = "CV")

PCR <- as.matrix(pcr_model$coefficients[49:56])
PCR <- round(rbind(mean(Ytrain), PCR),3)
PCR
```

**PLS column**
```{r}
train <- as.data.frame(cbind(Ytrain, Xtrain))
pls_model <- plsr(Ytrain ~ ., data = train)

PLS <- as.matrix(pls_model$coefficients[9:16])
PLS <- round(rbind(mean(Ytrain), PLS),3)
PLS
```

**Computing the test error**

```{r}
error <- function(mod){
  # We need to add the intercept, not multiply it
  Yhattest <- Xtest %*% mod[-1] + mod[1]
  Error <- Ytest - Yhattest
  # Testing error
  testError <- sum((Error)^2)/nrow(Xtest)
  # Training error
  stdError <- sd((Error)^2)/sqrt(nrow(Xtest))

  round(rbind(testError, stdError),3)
}
```

```{r}
#Adding the test and std error to each model
bo <- rbind(bo,error(bo))
bb <- rbind(bb, error(bb))
br <- rbind(br, error(br))
bl <- rbind(bl, error(bl))
PCR <- rbind(PCR, error(PCR))
PLS <- rbind(PLS, error(PLS))
```


**Making the table**
```{r}
table <- cbind(bo,bb,br,bl,PCR,PLS)
colnames(table) <- c('LS', '  Best Subset', '   Ridge', '   Lasso', '     PCR', '     PLS')
rownames(table)[1] <- c('Intercept')
rownames(table)[2:9] <- colnames(data)[1:8]
table
```

**Ridge**
```{r}
find_l <- function(lambda, Y, X){
Xtraincen <- X-mean(Xtrain)
br <- solve(t(Xtraincen) %*% Xtraincen + diag(x = lambda, ncol(Xtraincen)), t(Xtraincen) %*% (Y - mean(Y)))
br <- rbind(mean(Y),br)
br
}
Xtestone <- cbind(array(1, dim = c(nrow(Xtest),1)), Xtest)
find_l(24, Ytrain, Xtrain)
mse <- vector()
for (lambda in 1:50) {
  mse[lambda] <- sum((Ytest - Xtestone %*% find_l(lambda, Ytrain, Xtrain))^2)/nrow(Xtestone)
}
which.min(mse)

datf <- as.data.frame(cbind(c(1:50),mse))
colnames(datf)[1]<-'x'
pl <- ggplot(datf, aes(x,mse)) + geom_line()
pl
```

```{r}
library(caret)
library(dplyr)
folds <- createFolds(data, k = 10, returnTrain = TRUE)
lambda <- vector()

find_l <- function(lambda, Y, X){
Xtraincen <- (X - mean(Xtrain)) %>% as.matrix()
br <- solve(t(Xtraincen) %*% Xtraincen + diag(x = lambda, ncol(Xtraincen)), t(Xtraincen) %*% (Y - mean(Y)))
br <- rbind(mean(Y),br)
br
}
for(fold in 1:10){
  # Create the train and test datasets:
  Xtest <- data[folds[[fold]],-c(9,10)] %>% as.matrix()
  Xtestone <- cbind(array(1,dim = c(nrow(Xtest),1)), Xtest) %>% as.matrix()
  Ytest <- data[folds[[fold]],9] %>% as.matrix() %>% as.matrix()
  Xtrain <- data[-folds[[fold]], -c(9,10)] %>% as.matrix()
  Ytrain <- data[-folds[[fold]],9] %>% as.matrix()
  
  # Find lambda:
  mse <- vector()
  for(lambda in 1:30){
  mse[lambda]  <- sum((Ytest - Xtestone %*% find_l(lambda, Ytrain, Xtrain))^2)/nrow(Xtestone)
  }
  #print(which.min(mse[]))
  print(mse[])
}
```

```{r}
ridge <- function(Y,X,lambda){
  p <- 9
  beta_hat <- Variable(p-1)
  objective <- sum((scale(Y) - scale(X) %*% beta_hat)^2) + lambda*sum((beta_hat)^2)
  problem <- Problem(Minimize(objective))
  result <- solve(problem)
  
  d <- sqrt(diag(var(X)))
  b <- result$getValue(beta_hat)*sqrt(var(Y)) / d
  #b <- result$getValue(beta_hat)
  return(rbind(mean(Y),b))
}
```



```{r}
train_predictors <- predictors[train_set, ]
train_classes <- classes[train_set]
test_predictors <- predictors[-train_set, ]
test_classes <- classes[-train_set]
 
set.seed(seed)
cv_splits <- createFolds(data, k = 10, returnTrain = TRUE)
str(cv_splits)
cv_splits$Fold01

```
```{r}
for(lambda in 1:30){
  mse[lambda]  <- sum((Ytest - Xtestone %*% find_l(lambda, Ytrain, Xtrain))^2)/nrow(Xtestone)
  }
print(which.min(mse[]))
```

