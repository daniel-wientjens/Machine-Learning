---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r, message=FALSE}

library(CVXR)
library(leaps)
library(ggplot2)
library(pls)
library(caret)
```
#Ex 1.
1.
a) Reading in the data
```{r}
data <- read.table("http://statweb.stanford.edu/~tibs/ElemStatLearn.1stEd/datasets/prostate.data", sep = "")
```

```{r}
data[32,2] = 3.8044  # strange science 
```
b) Extract and normalize the explicative variables
```{r}
X <- scale(data[,1:8])
```
c) Is it wise to normalize these data?
d)Extract the target variable
```{r}
Y <- as.matrix(data[,"lpsa"])
```

e) Split the dataset into training and test data
```{r}
Xtrain <- X[data[["train"]], ]
Ytrain <- Y[data[["train"]], ]

Xtest <- X[!data[["train"]], ]
Ytest <- Y[!data[["train"]], ] 
```

**2. Compute the correlations of predictors in the prostate cancer data as presented Table 3.1**
```{r}
Xtrainscale <- scale(Xtrain)
#Needed for later excersizes
Ytrainscale <- scale(Ytrain) 
C <- cov(as.matrix(Xtrainscale))
```
3. Reproduce the results presented Table 3.2
a) Compute the coefficients of the linear regression model, without using the lm function (but you can use it validate your code)
```{r}
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain)
b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
```
Now we produce the linear regression model to compare
```{r}
lm0 <- lm(Ytrain ~Xtrain)
comp <-cbind(b,lm0$coefficients)
```

b) Compute the prediction error
```{r}
Ypred <- Xtrainone %*% b
err <- Ytrain - Ypred
```
c) Compute the standard error for each variable
```{r}
sig2 <- (t(err) %*% err)/ (nrow(Xtrainone) - ncol(X) -1)
v <- diag(solve(t(Xtrainone) %*% Xtrainone))
stderr <- sqrt(as.vector(sig2)) * sqrt(v)
```
d) compute the Z score for each variable
```{r}
Z <- b/stderr
```
e) visualize the results and compare with table 3.2
```{r}
table32 <- cbind(b,stderr,Z)
round(100*table32)/100
```


#Ex 2.
Reproduce Table 3.3, at least the first four columns that is LS, Best Subset, Ridge and Lasso.

First we use the CVXR package to calculate the coefficients found in table 3.3, to find these values we have to pose a problem and the constraints and solve them under a least-squares method of calculating a solution. This is then compared to the value obtained by running a linear regression on all the available regressors to check if we indeed get the same values.
As with a least-squares methods we try to minimize the difference between the predicted data of our model and the actual data in the training set.
#LS column
```{r}
# CVXR: An R Package for Disciplined Convex Optimization
p <- 9
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)
bo <- round(result$getValue(betaHat), 3)
#Lastly we compare the found bo model with the b model from exercise 1 to see if the values match up
cbind((round(1000*bo)/1000),(round(1000*b)/1000))
```


#Best subset column
Finding the best subset can be done in several different methods ranging from forward and backward elimination to sequential replacement. We have chosen for the exhaustive search method though.
```{r}
#Here we use the leaps package to find the best subset using an exhaustive search
df <- as.data.frame(cbind(Xtrain,Ytrain))
best.subset <- regsubsets(Ytrain~., df)
best.subset.summary <- summary(best.subset)
best.subset.summary$outmat
```
Since we already know that we only need two explanatory variables we use 'lcavol' and 'lweight' according to the table since they have the stars in the second row.
For good measure we calculate the solution with the CVXR package method as well as the least squares method to check if we get the same results again.
We take p to be 3 since we want two explanatory variables and the intercept.
```{r}
p <- 3
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone[,1:3] %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)
bb <- result$getValue(betaHat)
bb
```

Just to be sure we should only use two explanatory variables we can check some information criteria.

The adjusted R^2 shows us that it is maximized when we choose 7 variables and explains 66% of the variance, if we choose only 2 explanatory variables we explain roughly 60% of the variance. The added 6% of explained variance is not worth the complexity that the 7 explanatory variable regression model bring with it, if we just two regressors we explain only a bit less but have a much simpeler model which is easier to interprete.
Then looking at Mallow's CP since we use a least-square method once we found the best variables, here we also see that it defines the best subset model as the one containing 7 explanatory variables. But since we have 8 parameters in the full model we can say that there was a sampling error and hence we disregard this IC.
In the end we choose the Bayesian Information Criterion and indeed we can see below that we only need two explanatory variables since it minimizes the BIC.

```{r}
best.subset.by.bic <- which.min(best.subset.summary$bic)
best.subset.by.cp <- which.min(best.subset.summary$cp)
best.subset.by.adjr2 <- which.max(best.subset.summary$adjr2)

par(mfrow=c(2,2))
plot(best.subset$rss, xlab="Number of Variables", ylab="RSS", type="l")
plot(best.subset.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
points(best.subset.by.adjr2, best.subset.summary$adjr2[best.subset.by.adjr2], col="red", cex =2, pch =20)
plot(best.subset.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
points(best.subset.by.bic, best.subset.summary$bic[best.subset.by.bic], col="red", cex =2, pch =20)
plot(best.subset.summary$cp, xlab="Number of Variables", ylab="CP", type="l")
points(best.subset.by.cp, best.subset.summary$cp[best.subset.by.cp], col="red", cex =2, pch =20)
```
Next we use the same method employed during the LS model to find our coefficients only this time we use just the two explanatory variables the best subset model gave us.

```{r}
#Lastly we compare the found bb model with the lm1 model which used a linear regression on only the two explanatory variables found
lm1 <- lm(Ytrain ~Xtrain[,1:2])
comp <-cbind(bb,lm1$coefficients)
round(1000*comp)/1000
bb <- round(rbind(bb, 0, 0, 0, 0, 0, 0),3)
```

#Ridge column
First we have to centre the data since it is adviced in the book 'Elements of Statistical Learning'. The lambda penalty term is taken to be 24 from the partial solution given in the matlab code.
```{r}
#First off we centre the Xtrain data to account for the splitting of the dataset into test and train
Xtraincen <- Xtrain-mean(Xtrain)
lambda <- 24
#p get set back to 8 since we want to use all the regressors available in the model
p<- 8
#Here our constraint has changed into minimizing the difference between the centred and scaled Ytraining data and the predicted Y training data while taking the penalty term into account
betaHat <- Variable(p)
objective <- Minimize(sum(((Ytrainscale-mean(Ytrainscale)) - Xtrainscale %*% betaHat)^2) + lambda*sum((betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)
d <- sqrt(diag(var(Xtrainscale)))    
br <- round(rbind(mean(Ytrain),result$getValue(betaHat)*sqrt(var(Ytrain)) / d),3)
br
```

Unfortunately we do not obtain the correct values of the coefficients found in the table. Next we check if we get the same values when we use the 'normal' method to get the solution and we indeed get a lot closer.

```{r}
br <- solve(t(Xtraincen) %*% Xtraincen + diag(x = lambda, ncol(Xtraincen)), t(Xtraincen) %*% (Ytrain - mean(Ytrain)))
br <- rbind(mean(Ytrain),br)
br <- round(br,3)
br
```


#The Lasso
Again it is smart to use the scaled data. And remember that p is set to 8 from the ridge computation, this is used here again since the Ridge and Lasso method are very closely related. The only difference is that the regularisation term in the lasso computation is set to be an absolut value. This small adjustment has big consequences for the outcomes which can be seen in table 3.3
```{r}
#the value for t is taken from the solution provided in matlab code
t <-  .7015
ys = scale(Ytrain)
betaHat <- Variable(p)
objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
constraint <- list(sum(abs(betaHat)) <=  t)
problem <- Problem(objective, constraint)
result <- solve(problem)

d <- sqrt(diag(var(Xtrain)))
bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
bl <- round(rbind(mean(Ypred-(Xtrain %*% bl)),bl),3)
bl
```



#PCR column
```{r}
train <- as.data.frame(cbind(Ytrain, Xtrain))
pcr_model <- pcr(Ytrain ~ ., data = train, validation = "CV")

PCR <- as.matrix(pcr_model$coefficients[49:56])
PCR <- round(rbind(mean(Ytrain), PCR),3)
PCR
```

#PLS column
```{r}
train <- as.data.frame(cbind(Ytrain, Xtrain))
pls_model <- plsr(Ytrain ~ ., data = train)

PLS <- as.matrix(pls_model$coefficients[9:16])
PLS <- round(rbind(mean(Ytrain), PLS),3)
PLS
```

#Computing the test error
In order to calculate the error per model in a quick manner I made a function where one can put the coefficients per model to obtain the test error and the std error.
```{r}
error <- function(mod){
#Here we multiply the test set with the found coefficients except the intercept, which we add immediately later after that
  Yhattest <- Xtest %*% mod[-1] + mod[1]
  error <- Ytest - Yhattest
#Definition of Testing error
  test_error <- sum((error)^2)/nrow(Xtest)
#Definition of Std error
  std_error <- sd((error)^2)/sqrt(nrow(Xtest))

  round(rbind(test_error, std_error),3)
}
```

```{r}
#Adding the test and std error to each model
bo <- rbind(bo,error(bo))
bb <- rbind(bb, error(bb))
br <- rbind(br, error(br))
bl <- rbind(bl, error(bl))
PCR <- rbind(PCR, error(PCR))
PLS <- rbind(PLS, error(PLS))
```


#Making the table
```{r}
table <- cbind(bo,bb,br,bl,PCR,PLS)
colnames(table) <- c('LS', '  Best Subset', '   Ridge', '   Lasso', '     PCR', '     PLS')
rownames(table)[1] <- c('Intercept')
rownames(table)[2:9] <- colnames(data)[1:8]
table
```



#Attempt to justify lambda = 24 in the ridge column with cross-validation
**Ridge**
```{r}
find_l <- function(lambda, Y, X){
Xtraincen <- X-mean(Xtrain)
br <- solve(t(Xtraincen) %*% Xtraincen + diag(x = lambda, ncol(Xtraincen)), t(Xtraincen) %*% (Y - mean(Y)))
br <- rbind(mean(Y),br)
br
}
Xtestone <- cbind(array(1, dim = c(nrow(Xtest),1)), Xtest)
find_l(24, Ytrain, Xtrain)
mse <- vector()
for (lambda in 1:50) {
  mse[lambda] <- sum((Ytest - Xtestone %*% find_l(lambda, Ytrain, Xtrain))^2)/nrow(Xtestone)
}
which.min(mse)

datf <- as.data.frame(cbind(c(1:50),mse))
colnames(datf)[1]<-'x'
pl <- ggplot(datf, aes(x,mse)) + geom_line()
pl
```
Here we have found that the mse is minimized when a lambda of 14 is used which actually provides a better fit than the model proposed in for ridge in table 3.3
```{r}
mse <- sum((Ytest - Xtestone %*% find_l(14, Ytrain, Xtrain))^2)/nrow(Xtestone)
mse
mse <- sum((Ytest - Xtestone %*% find_l(24, Ytrain, Xtrain))^2)/nrow(Xtestone)
mse
```

Next a CV is attempted because maybe lambda = 14 is the best lambda because the way the data is split.
Unfortunately this is where I got stuck on the fact that all my training sets propose a lambda of 1.
```{r}
library(caret)
library(dplyr)
folds <- createFolds(data, k = 10, returnTrain = TRUE)
lambda <- vector()

find_l <- function(lambda, Y, X){
Xtraincen <- (X - mean(Xtrain)) %>% as.matrix()
br <- solve(t(Xtraincen) %*% Xtraincen + diag(x = lambda, ncol(Xtraincen)), t(Xtraincen) %*% (Y - mean(Y)))
br <- rbind(mean(Y),br)
br
}
for(fold in 1:10){
  # Create the train and test datasets:
  Xtest <- data[folds[[fold]],-c(9,10)] %>% as.matrix()
  Xtestone <- cbind(array(1,dim = c(nrow(Xtest),1)), Xtest) %>% as.matrix()
  Ytest <- data[folds[[fold]],9] %>% as.matrix() %>% as.matrix()
  Xtrain <- data[-folds[[fold]], -c(9,10)] %>% as.matrix()
  Ytrain <- data[-folds[[fold]],9] %>% as.matrix()
  
  # Find lambda:
  mse <- vector()
  for(lambda in 1:30){
  mse[lambda]  <- sum((Ytest - Xtestone %*% find_l(lambda, Ytrain, Xtrain))^2)/nrow(Xtestone)
  }
  print(which.min(mse[]))
  #print(mse[])
}
```


